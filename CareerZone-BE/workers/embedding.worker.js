import path from 'path';
import dotenv from 'dotenv';
dotenv.config({ path: path.resolve(process.cwd(), '.env') });

import connectDB from '../src/utils/connectDB.js';
import logger from '../src/utils/logger.js';
import { Job } from '../src/models/index.js';

import {
  buildSearchText,
  splitWithOverlap,
  embedContent,
  hashSource
} from '../src/embeddings/helpers.js';


const INTEREST_FIELDS = [
  'title','description','requirements'
];
function isEmbeddingAffected(updatedFields = {}, removedFields = []) {
  const keys = Object.keys(updatedFields);
  const touched = new Set([
    ...keys.map(k => k.split('.')[0]),
    ...removedFields.map(k => k.split('.')[0]),
  ]);
  return INTEREST_FIELDS.some(f => touched.has(f.split('.')[0]));
}

async function processJobEmbedding(jobId) {
  const job = await Job.findById(jobId).lean();
  if (!job) return;

  const source = buildSearchText(job);
  const newHash = hashSource(source);

  const chunks = splitWithOverlap(source);
  if (chunks.length === 0) {
    await Job.findByIdAndUpdate(jobId, {
      $set: {
        chunks: [],
        embeddingsUpdatedAt: new Date(),
      }
    });
    logger.info(`ðŸ—‘ï¸ Job ${jobId} khÃ´ng cÃ³ ná»™i dung â†’ clear chunks`);
    return;
  }

  const requests = chunks.map(t => ({
    model: process.env.EMBED_MODEL || 'models/embedding-001',
    content: {
      parts: [{ text: t }],
    },
  }));
  const vectors = await embedContent(requests);

  const newChunks = vectors.map((e, i) => ({
    jobId,
    chunkIndex: i,
    pageContent: chunks[i],
    embedding: e?.embedding?.values || []
  }));

  await Job.findByIdAndUpdate(jobId, {
    $set: {
      chunks: newChunks,
      embeddingsUpdatedAt: new Date(),
    }
  });
  logger.info(`âœ… Re-embedded Job ${jobId}: ${newChunks.length} chunks`);
}

// ============ Start Change Streams ============
async function startWorker() {
  await connectDB();

  logger.info('ðŸš€ Embedding worker (Change Streams) started. Listening for DB changes...');

  const pipeline = [
    { $match: { operationType: { $in: ['insert', 'update'] } } },
    // CÃ³ thá»ƒ thÃªm: { $match: { 'fullDocument.status': 'PUBLISHED' } },
  ];

  let lastResumeToken = null;
  let changeStream = Job.collection.watch(pipeline, { fullDocument: 'updateLookup' });

  changeStream.on('change', async (change) => {
    try {
      lastResumeToken = change._id;

      const job = change.fullDocument;
      if (!job) {
        logger.warn('Change event thiáº¿u fullDocument', { change });
        return;
      }

      if (change.operationType === 'update') {
        const updatedFields = change.updateDescription?.updatedFields || {};
        const removedFields = change.updateDescription?.removedFields || [];
        if (!isEmbeddingAffected(updatedFields, removedFields)) {
          logger.info(`Job ${job._id} cáº­p nháº­t nhÆ°ng khÃ´ng áº£nh hÆ°á»Ÿng embedding â†’ bá» qua`);
          return;
        }
      }

      await processJobEmbedding(job._id.toString());
    } catch (err) {
      logger.error(`âŒ Lá»—i xá»­ lÃ½ change stream Job ${change.documentKey?._id}: ${err.message}`, { stack: err.stack });
    }
  });

  changeStream.on('error', async (error) => {
    logger.error('ðŸš¨ Change Stream lá»—i:', error);
    try { await changeStream.close(); } catch {}
    const opts = { fullDocument: 'updateLookup', ...(lastResumeToken ? { resumeAfter: lastResumeToken } : {}) };
    logger.warn('ðŸ” Restart Change Stream...');
    changeStream = Job.collection.watch(pipeline, opts);
  });

  changeStream.on('close', () => {
    logger.warn('âš ï¸ Change Stream Ä‘Ã³ng. NÃªn cÃ³ PM2/systemd tá»± restart tiáº¿n trÃ¬nh.');
  });
}

startWorker().catch(async (e) => {
  logger.error('ðŸš¨ Worker khá»Ÿi Ä‘á»™ng lá»—i', e);
  process.exit(1);
});